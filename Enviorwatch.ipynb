{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOadbGercH4KQoUdSvBlmQ2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nqfissa/Envirowatch/blob/main/Enviorwatch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "AV67VAemCGIt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3705f479"
      },
      "source": [
        "# Task\n",
        "Develop environmental hazard detection (CNN classifier for 'deforested' vs 'forest' image patches, upgrading to U-Net for semantic segmentation) and wildlife detection systems (image classification for species presence/absence, upgrading to object detection with YOLOv8 or Faster R-CNN for bounding box predictions)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6272940"
      },
      "source": [
        "## Environmental Hazard Detection (CNN Classifier)\n",
        "\n",
        "### Subtask:\n",
        "Develop and train a simpler CNN classifier to distinguish between 'deforested' and 'forest' image patches.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "427fa61a"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step in developing the CNN classifier is to load and prepare the image dataset. This involves defining the image dimensions, batch size, and using TensorFlow's `image_dataset_from_directory` to load images from specified directories, automatically label them, and split them into training and validation sets while rescaling pixel values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a0ffe75",
        "outputId": "a720b98b-b24c-48d7-e961-3b33489df30c"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define image dimensions and batch size\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Define paths to your dataset directories\n",
        "# Assuming 'deforested' and 'forest' are subdirectories within 'image_dataset_path'\n",
        "# You might need to adjust this path based on your environment\n",
        "data_dir = 'image_dataset_path'\n",
        "\n",
        "# Load and prepare the dataset\n",
        "# The dataset will be automatically labeled based on subdirectory names\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    image_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "    interpolation='nearest',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    subset='training',\n",
        "    validation_split=0.2, # 20% for validation\n",
        "    seed=42 # for reproducibility\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    image_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "    interpolation='nearest',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    subset='validation',\n",
        "    validation_split=0.2, # 20% for validation\n",
        "    seed=42 # for reproducibility\n",
        ")\n",
        "\n",
        "# Rescale pixel values to [0, 1]\n",
        "def rescale(image, label):\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image, label\n",
        "\n",
        "train_ds = train_ds.map(rescale)\n",
        "val_ds = val_ds.map(rescale)\n",
        "\n",
        "# Optionally, print some information about the datasets\n",
        "print(f\"Number of training batches: {tf.data.experimental.cardinality(train_ds).numpy()}\")\n",
        "print(f\"Number of validation batches: {tf.data.experimental.cardinality(val_ds).numpy()}\")\n",
        "print(\"Dataset loaded and prepared.\")\n",
        "\n",
        "# Prefetching for performance\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10 files belonging to 2 classes.\n",
            "Using 8 files for training.\n",
            "Found 10 files belonging to 2 classes.\n",
            "Using 2 files for validation.\n",
            "Number of training batches: 1\n",
            "Number of validation batches: 1\n",
            "Dataset loaded and prepared.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afd1bf72"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the `image_dataset_path` directory was not found. To resolve this, I will create dummy directories and empty image files to simulate the expected dataset structure, allowing `image_dataset_from_directory` to run without error.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0761b549",
        "outputId": "c9db6a2c-c8fc-4bbb-92af-e293b063da69"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Define image dimensions and batch size\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Define paths to your dataset directories\n",
        "data_dir = 'image_dataset_path'\n",
        "\n",
        "# Create dummy directories and files if they don't exist\n",
        "# This is a placeholder to make the code runnable without actual data\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(os.path.join(data_dir, 'deforested'))\n",
        "    os.makedirs(os.path.join(data_dir, 'forest'))\n",
        "    # Create a few dummy files in each directory\n",
        "    for i in range(5):\n",
        "        with open(os.path.join(data_dir, 'deforested', f'def_{i}.jpg'), 'w') as f:\n",
        "            f.write('')\n",
        "        with open(os.path.join(data_dir, 'forest', f'for_{i}.jpg'), 'w') as f:\n",
        "            f.write('')\n",
        "    print(f\"Dummy directories and files created in {data_dir}\")\n",
        "\n",
        "# Load and prepare the dataset\n",
        "# The dataset will be automatically labeled based on subdirectory names\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    image_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "    interpolation='nearest',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    subset='training',\n",
        "    validation_split=0.2, # 20% for validation\n",
        "    seed=42 # for reproducibility\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    image_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "    interpolation='nearest',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    subset='validation',\n",
        "    validation_split=0.2, # 20% for validation\n",
        "    seed=42 # for reproducibility\n",
        ")\n",
        "\n",
        "# Rescale pixel values to [0, 1]\n",
        "def rescale(image, label):\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image, label\n",
        "\n",
        "train_ds = train_ds.map(rescale)\n",
        "val_ds = val_ds.map(rescale)\n",
        "\n",
        "# Optionally, print some information about the datasets\n",
        "print(f\"Number of training batches: {tf.data.experimental.cardinality(train_ds).numpy()}\")\n",
        "print(f\"Number of validation batches: {tf.data.experimental.cardinality(val_ds).numpy()}\")\n",
        "print(\"Dataset loaded and prepared.\")\n",
        "\n",
        "# Prefetching for performance\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10 files belonging to 2 classes.\n",
            "Using 8 files for training.\n",
            "Found 10 files belonging to 2 classes.\n",
            "Using 2 files for validation.\n",
            "Number of training batches: 1\n",
            "Number of validation batches: 1\n",
            "Dataset loaded and prepared.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b2a8446"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the dataset is loaded and prepared, the next step is to define the CNN model architecture as per the instructions. This involves creating a Sequential model with Conv2D, MaxPooling2D, Flatten, and Dense layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "a71483ad",
        "outputId": "735986a1-1699-469e-c35e-56bf3bcf7bce"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define the CNN model architecture\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid') # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "print(\"CNN model architecture defined.\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_6 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_7 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_8 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m3,211,392\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,211,392</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,304,769\u001b[0m (12.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,304,769</span> (12.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,304,769\u001b[0m (12.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,304,769</span> (12.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN model architecture defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "938be959"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the CNN model architecture has been defined, the next step is to compile the model. This involves specifying the optimizer, loss function, and metrics as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63c97cf5",
        "outputId": "a7bb5e4a-fad4-4faf-859b-8958abc893e1"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define the CNN model architecture\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid') # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"CNN model compiled.\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN model compiled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe5bd7a5"
      },
      "source": [
        "**Reasoning**:\n",
        "With the model architecture defined and compiled, the next logical step is to train the model using the prepared training dataset and evaluate its performance on the validation dataset over a specified number of epochs, as outlined in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02eb6a56",
        "outputId": "5f987ece-1bf7-488e-e2f8-268eb2dcc9c6"
      },
      "source": [
        "EPOCHS = 10 # You can adjust this number\n",
        "\n",
        "print(\"Training the model...\")\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_ds\n",
        ")\n",
        "\n",
        "print(\"Model training complete.\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model...\n",
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 458ms/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 650ms/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 432ms/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 615ms/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329ms/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9160005"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous training step failed because the dummy image files were empty, causing an `InvalidArgumentError` during image decoding. To resolve this, I will modify the dummy file creation to generate actual small, blank JPEG images using Pillow, ensuring that `image_dataset_from_directory` can properly read and decode them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09f2acda",
        "outputId": "19f9eec2-7907-4a0f-beb1-c4ede7f1a4c1"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Define image dimensions and batch size\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Define paths to your dataset directories\n",
        "data_dir = 'image_dataset_path'\n",
        "\n",
        "# Create dummy directories and files if they don't exist\n",
        "# This is a placeholder to make the code runnable without actual data\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(os.path.join(data_dir, 'deforested'))\n",
        "    os.makedirs(os.path.join(data_dir, 'forest'))\n",
        "\n",
        "    # Create a function to generate a dummy image\n",
        "    def create_dummy_image(path, width, height):\n",
        "        img = Image.fromarray(np.zeros((height, width, 3), dtype=np.uint8))\n",
        "        img.save(path)\n",
        "\n",
        "    # Create a few dummy files in each directory\n",
        "    for i in range(5):\n",
        "        create_dummy_image(os.path.join(data_dir, 'deforested', f'def_{i}.jpg'), IMG_WIDTH, IMG_HEIGHT)\n",
        "        create_dummy_image(os.path.join(data_dir, 'forest', f'for_{i}.jpg'), IMG_WIDTH, IMG_HEIGHT)\n",
        "    print(f\"Dummy directories and valid image files created in {data_dir}\")\n",
        "\n",
        "# Load and prepare the dataset\n",
        "# The dataset will be automatically labeled based on subdirectory names\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    image_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "    interpolation='nearest',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    subset='training',\n",
        "    validation_split=0.2, # 20% for validation\n",
        "    seed=42 # for reproducibility\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    image_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "    interpolation='nearest',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    subset='validation',\n",
        "    validation_split=0.2, # 20% for validation\n",
        "    seed=42 # for reproducibility\n",
        ")\n",
        "\n",
        "# Rescale pixel values to [0, 1]\n",
        "def rescale(image, label):\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image, label\n",
        "\n",
        "train_ds = train_ds.map(rescale)\n",
        "val_ds = val_ds.map(rescale)\n",
        "\n",
        "# Optionally, print some information about the datasets\n",
        "print(f\"Number of training batches: {tf.data.experimental.cardinality(train_ds).numpy()}\")\n",
        "print(f\"Number of validation batches: {tf.data.experimental.cardinality(val_ds).numpy()}\")\n",
        "print(\"Dataset loaded and prepared.\")\n",
        "\n",
        "# Prefetching for performance\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10 files belonging to 2 classes.\n",
            "Using 8 files for training.\n",
            "Found 10 files belonging to 2 classes.\n",
            "Using 2 files for validation.\n",
            "Number of training batches: 1\n",
            "Number of validation batches: 1\n",
            "Dataset loaded and prepared.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39d67bea"
      },
      "source": [
        "**Reasoning**:\n",
        "With the model trained using the prepared datasets, the final step is to evaluate its performance on the validation set to determine its accuracy and loss, as specified in the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e90080d",
        "outputId": "3ff54f4f-55a1-43ae-d670-cf2c55b6530a"
      },
      "source": [
        "print(\"Evaluating the model...\")\n",
        "loss, accuracy = model.evaluate(val_ds)\n",
        "\n",
        "print(f\"Validation Loss: {loss:.4f}\")\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "print(\"Model evaluation complete.\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating the model...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.5000 - loss: 0.6931\n",
            "Validation Loss: 0.6931\n",
            "Validation Accuracy: 0.5000\n",
            "Model evaluation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9de0737"
      },
      "source": [
        "**Reasoning**:\n",
        "The model evaluation failed with an `InvalidArgumentError` because the previously created dummy image files might still be empty or corrupted from earlier runs, preventing proper decoding. To ensure fresh, valid dummy image files are used, I will first remove the `data_dir` if it exists, and then re-execute the code that creates new, valid dummy JPEG images and loads the dataset. This will guarantee that the dataset used for evaluation contains properly formed images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac455602",
        "outputId": "1aecce1f-5284-4a33-b911-b33521147b56"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import shutil # Import shutil for directory removal\n",
        "\n",
        "# Define image dimensions and batch size\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Define paths to your dataset directories\n",
        "data_dir = 'image_dataset_path'\n",
        "\n",
        "# Ensure clean slate: remove data_dir if it exists to regenerate valid dummy images\n",
        "if os.path.exists(data_dir):\n",
        "    shutil.rmtree(data_dir)\n",
        "    print(f\"Removed existing data directory: {data_dir}\")\n",
        "\n",
        "# Create dummy directories and files if they don't exist\n",
        "# This is a placeholder to make the code runnable without actual data\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(os.path.join(data_dir, 'deforested'))\n",
        "    os.makedirs(os.path.join(data_dir, 'forest'))\n",
        "\n",
        "    # Create a function to generate a dummy image\n",
        "    def create_dummy_image(path, width, height):\n",
        "        img = Image.fromarray(np.zeros((height, width, 3), dtype=np.uint8))\n",
        "        img.save(path)\n",
        "\n",
        "    # Create a few dummy files in each directory\n",
        "    for i in range(5):\n",
        "        create_dummy_image(os.path.join(data_dir, 'deforested', f'def_{i}.jpg'), IMG_WIDTH, IMG_HEIGHT)\n",
        "        create_dummy_image(os.path.join(data_dir, 'forest', f'for_{i}.jpg'), IMG_WIDTH, IMG_HEIGHT)\n",
        "    print(f\"Dummy directories and valid image files created in {data_dir}\")\n",
        "\n",
        "# Load and prepare the dataset\n",
        "# The dataset will be automatically labeled based on subdirectory names\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    image_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "    interpolation='nearest',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    subset='training',\n",
        "    validation_split=0.2, # 20% for validation\n",
        "    seed=42 # for reproducibility\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='binary',\n",
        "    image_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "    interpolation='nearest',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    subset='validation',\n",
        "    validation_split=0.2, # 20% for validation\n",
        "    seed=42 # for reproducibility\n",
        ")\n",
        "\n",
        "# Rescale pixel values to [0, 1]\n",
        "def rescale(image, label):\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image, label\n",
        "\n",
        "train_ds = train_ds.map(rescale)\n",
        "val_ds = val_ds.map(rescale)\n",
        "\n",
        "# Optionally, print some information about the datasets\n",
        "print(f\"Number of training batches: {tf.data.experimental.cardinality(train_ds).numpy()}\")\n",
        "print(f\"Number of validation batches: {tf.data.experimental.cardinality(val_ds).numpy()}\")\n",
        "print(\"Dataset loaded and prepared.\")\n",
        "\n",
        "# Prefetching for performance\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed existing data directory: image_dataset_path\n",
            "Dummy directories and valid image files created in image_dataset_path\n",
            "Found 10 files belonging to 2 classes.\n",
            "Using 8 files for training.\n",
            "Found 10 files belonging to 2 classes.\n",
            "Using 2 files for validation.\n",
            "Number of training batches: 1\n",
            "Number of validation batches: 1\n",
            "Dataset loaded and prepared.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f335265"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the dataset has been re-prepared with valid dummy images, the model needs to be re-trained. This step is crucial to ensure the model learns from correctly decoded images and to resolve the `InvalidArgumentError` encountered during the previous training attempt.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97f71b9b",
        "outputId": "cf374f5b-d189-4523-b72b-70ae948c937f"
      },
      "source": [
        "EPOCHS = 10 # You can adjust this number\n",
        "\n",
        "print(\"Training the model...\")\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_ds\n",
        ")\n",
        "\n",
        "print(\"Model training complete.\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model...\n",
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314ms/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 286ms/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310ms/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16a1aad3"
      },
      "source": [
        "**Reasoning**:\n",
        "With the model now re-trained using valid dummy images, the next and final step for this subtask is to evaluate its performance on the validation set to obtain its final accuracy and loss, as specified in the original instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "455436a1",
        "outputId": "0c838ccc-cba3-4b59-e021-07e7b6624090"
      },
      "source": [
        "print(\"Evaluating the model...\")\n",
        "loss, accuracy = model.evaluate(val_ds)\n",
        "\n",
        "print(f\"Validation Loss: {loss:.4f}\")\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "print(\"Model evaluation complete.\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating the model...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.5000 - loss: 0.6931\n",
            "Validation Loss: 0.6931\n",
            "Validation Accuracy: 0.5000\n",
            "Model evaluation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e67b73a6"
      },
      "source": [
        "# Task\n",
        "Generate dummy image and corresponding mask files for the U-Net model within the 'images' and 'masks' directories, to simulate a dataset for training the U-Net model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7029b1fb"
      },
      "source": [
        "## Generate Dummy Image-Mask Pairs\n",
        "\n",
        "### Subtask:\n",
        "Generate dummy image and corresponding mask files (e.g., JPEG or PNG) within the 'images' and 'masks' directories. These dummy files will simulate a dataset for training the U-Net model, where masks contain pixel-wise labels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e161dd88"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to generate dummy image-mask pairs to simulate a dataset for U-Net training. This involves importing necessary libraries, defining constants, creating the required directory structure, and then programmatically generating and saving placeholder images and corresponding masks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59cce537",
        "outputId": "d762a10b-8f5b-4650-b726-d6db26108c67"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import shutil\n",
        "\n",
        "# Define constants\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "NUM_SAMPLES = 10\n",
        "BASE_DIR = 'unet_dataset'\n",
        "\n",
        "# Define paths for images and masks\n",
        "images_dir = os.path.join(BASE_DIR, 'images')\n",
        "masks_dir = os.path.join(BASE_DIR, 'masks')\n",
        "\n",
        "# Ensure a clean slate: remove existing directories if they exist\n",
        "if os.path.exists(BASE_DIR):\n",
        "    shutil.rmtree(BASE_DIR)\n",
        "    print(f\"Removed existing directory: {BASE_DIR}\")\n",
        "\n",
        "# Create base directory and subdirectories for images and masks\n",
        "os.makedirs(images_dir, exist_ok=True)\n",
        "os.makedirs(masks_dir, exist_ok=True)\n",
        "print(f\"Created directories: {images_dir} and {masks_dir}\")\n",
        "\n",
        "# Generate dummy image and mask files\n",
        "for i in range(NUM_SAMPLES):\n",
        "    # Generate dummy image (random RGB data)\n",
        "    dummy_image_array = np.random.randint(0, 256, (IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.uint8)\n",
        "    dummy_image = Image.fromarray(dummy_image_array)\n",
        "    image_filename = os.path.join(images_dir, f'image_{i:03d}.jpg')\n",
        "    dummy_image.save(image_filename)\n",
        "\n",
        "    # Generate dummy mask (random grayscale data with 0 or 1 for classes)\n",
        "    # Assuming 2 classes: 0 for 'forest', 1 for 'deforested'\n",
        "    dummy_mask_array = np.random.randint(0, 2, (IMG_HEIGHT, IMG_WIDTH), dtype=np.uint8)\n",
        "    dummy_mask = Image.fromarray(dummy_mask_array, mode='L') # 'L' mode for single channel grayscale\n",
        "    mask_filename = os.path.join(masks_dir, f'mask_{i:03d}.png')\n",
        "    dummy_mask.save(mask_filename)\n",
        "\n",
        "print(f\"Successfully generated {NUM_SAMPLES} dummy image-mask pairs in '{BASE_DIR}'.\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed existing directory: unet_dataset\n",
            "Created directories: unet_dataset/images and unet_dataset/masks\n",
            "Successfully generated 10 dummy image-mask pairs in 'unet_dataset'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1695421087.py:37: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  dummy_mask = Image.fromarray(dummy_mask_array, mode='L') # 'L' mode for single channel grayscale\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7f71c9c"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the dummy image and mask files have been successfully generated, the next logical step is to create a TensorFlow dataset to load and preprocess these image-mask pairs for U-Net training. This involves creating a function to load and preprocess individual image and mask files, and then using `tf.data.Dataset` to create the training and validation datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "ab303c0f",
        "outputId": "0515ea39-438b-4577-a40c-bbb97aeb536a"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define image dimensions and batch size (already defined, but including for completeness)\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "BATCH_SIZE = 32\n",
        "BASE_DIR = 'unet_dataset'\n",
        "\n",
        "# Define paths for images and masks\n",
        "images_dir = os.path.join(BASE_DIR, 'images')\n",
        "masks_dir = os.path.join(BASE_DIR, 'masks')\n",
        "\n",
        "# Function to load and preprocess image and mask\n",
        "def load_image_mask(image_path, mask_path):\n",
        "    # Load image\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])\n",
        "    image = tf.cast(image, tf.float32) / 255.0 # Rescale to [0, 1]\n",
        "\n",
        "    # Load mask\n",
        "    mask = tf.io.read_file(mask_path)\n",
        "    # Use decode_png for masks as they were saved as PNG\n",
        "    mask = tf.image.decode_png(mask, channels=1)\n",
        "    mask = tf.image.resize(mask, [IMG_HEIGHT, IMG_WIDTH], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    mask = tf.cast(mask, tf.uint8) # Masks typically kept as int/uint for class labels\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "# Get lists of image and mask paths\n",
        "image_paths = sorted([os.path.join(images_dir, fname) for fname in os.listdir(images_dir) if fname.endswith('.jpg')])\n",
        "mask_paths = sorted([os.path.join(masks_dir, fname) for fname in os.listdir(masks_dir) if fname.endswith('.png')])\n",
        "\n",
        "# Create a TensorFlow Dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))\n",
        "dataset = dataset.map(load_image_mask, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Shuffle, batch, and prefetch the dataset\n",
        "BUFFER_SIZE = tf.data.AUTOTUNE # Adjust based on dataset size\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# Split into training and validation datasets (e.g., 80/20 split)\n",
        "DATASET_SIZE = tf.data.experimental.cardinality(dataset).numpy() * BATCH_SIZE # Approximate total samples\n",
        "TRAIN_SIZE = int(0.8 * DATASET_SIZE)\n",
        "VAL_SIZE = DATASET_SIZE - TRAIN_SIZE\n",
        "\n",
        "train_dataset = dataset.take(TRAIN_SIZE // BATCH_SIZE)\n",
        "val_dataset = dataset.skip(TRAIN_SIZE // BATCH_SIZE)\n",
        "\n",
        "print(f\"Successfully created training and validation datasets. Number of training batches: {tf.data.experimental.cardinality(train_dataset).numpy()}, Number of validation batches: {tf.data.experimental.cardinality(val_dataset).numpy()}.\")\n",
        "\n",
        "# Optional: inspect a batch\n",
        "# for image_batch, mask_batch in train_dataset.take(1):\n",
        "#     print(f\"Image batch shape: {image_batch.shape}\")\n",
        "#     print(f\"Mask batch shape: {mask_batch.shape}\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "{{function_node __wrapped__ShuffleDatasetV3_device_/job:localhost/replica:0/task:0/device:CPU:0}} buffer_size must be greater than zero or UNKNOWN_CARDINALITY [Op:ShuffleDatasetV3] name: ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2317868490.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Shuffle, batch, and prefetch the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mBUFFER_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m \u001b[0;31m# Adjust based on dataset size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUFFER_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Split into training and validation datasets (e.g., 80/20 split)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mshuffle\u001b[0;34m(self, buffer_size, seed, reshuffle_each_iteration, name)\u001b[0m\n\u001b[1;32m   1508\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtransformation\u001b[0m \u001b[0mapplied\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdescribed\u001b[0m \u001b[0mabove\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \"\"\"\n\u001b[0;32m-> 1510\u001b[0;31m     return shuffle_op._shuffle(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   1511\u001b[0m         self, buffer_size, seed, reshuffle_each_iteration, name=name)\n\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/shuffle_op.py\u001b[0m in \u001b[0;36m_shuffle\u001b[0;34m(input_dataset, buffer_size, seed, reshuffle_each_iteration, name)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m ):\n\u001b[0;32m---> 32\u001b[0;31m   return _ShuffleDataset(\n\u001b[0m\u001b[1;32m     33\u001b[0m       input_dataset, buffer_size, seed, reshuffle_each_iteration, name=name)\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/shuffle_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, buffer_size, seed, reshuffle_each_iteration, name)\u001b[0m\n\u001b[1;32m     55\u001b[0m     if (tf2.enabled() and\n\u001b[1;32m     56\u001b[0m         (context.executing_eagerly() or ops.inside_function())):\n\u001b[0;32m---> 57\u001b[0;31m       variant_tensor = gen_dataset_ops.shuffle_dataset_v3(\n\u001b[0m\u001b[1;32m     58\u001b[0m           \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           \u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mshuffle_dataset_v3\u001b[0;34m(input_dataset, buffer_size, seed, seed2, seed_generator, output_types, output_shapes, reshuffle_each_iteration, metadata, name)\u001b[0m\n\u001b[1;32m   7198\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7199\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7200\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7201\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7202\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6004\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6005\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6006\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__ShuffleDatasetV3_device_/job:localhost/replica:0/task:0/device:CPU:0}} buffer_size must be greater than zero or UNKNOWN_CARDINALITY [Op:ShuffleDatasetV3] name: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f54b427"
      },
      "source": [
        "## U-Net Segmentation Data Preparation\n",
        "\n",
        "This section details the preparation of the dataset for U-Net segmentation, including the generation of Sentinel-2-like multispectral image data and corresponding NDVI-derived masks, followed by the creation of TensorFlow `tf.data.Dataset` objects for training and validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5a82224"
      },
      "source": [
        "### 1. Generate Sentinel-2-like Image and NDVI-Derived Mask Files\n",
        "\n",
        "This step creates a dataset consisting of 4-channel, 16-bit NumPy arrays (simulating Sentinel-2 multispectral images) and 1-channel, 8-bit PNG masks. For the first sample, actual Sentinel-2 GeoTIFFs are processed, and their masks are derived using NDVI. For subsequent samples, dummy images are generated, and their masks are also derived from NDVI calculation. This ensures a consistent and relevant mask generation strategy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f2ec30e"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import shutil\n",
        "import rasterio\n",
        "\n",
        "# Define constants\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "NUM_SAMPLES = 50 # Total number of samples for the dataset\n",
        "BASE_DIR = 'unet_dataset'\n",
        "\n",
        "# Define paths for images and masks\n",
        "images_dir = os.path.join(BASE_DIR, 'images')\n",
        "masks_dir = os.path.join(BASE_DIR, 'masks')\n",
        "\n",
        "# Ensure a clean slate: remove existing directories if they exist\n",
        "if os.path.exists(BASE_DIR):\n",
        "    shutil.rmtree(BASE_DIR)\n",
        "    print(f\"Removed existing directory: {BASE_DIR}\")\n",
        "\n",
        "# Create base directory and subdirectories for images and masks\n",
        "os.makedirs(images_dir, exist_ok=True)\n",
        "os.makedirs(masks_dir, exist_ok=True)\n",
        "print(f\"Created directories: {images_dir} and {masks_dir}\")\n",
        "\n",
        "# Paths to the provided Sentinel-2 GeoTIFF files\n",
        "sentinel_band_paths = {\n",
        "    'B02': '/content/2023-02-04-00_00_2023-05-25-23_59_Sentinel-2_L1C_B02_(Raw).tiff',\n",
        "    'B03': '/content/2023-02-04-00_00_2023-05-25-23_59_Sentinel-2_L1C_B03_(Raw).tiff',\n",
        "    'B04': '/content/2023-02-04-00_00_2023-05-25-23_59_Sentinel-2_L1C_B04_(Raw).tiff',\n",
        "    'B08': '/content/2023-02-04-00_00_2023-05-25-23_59_Sentinel-2_L1C_B08_(Raw).tiff'\n",
        "}\n",
        "\n",
        "# Generate image and NDVI-derived mask files\n",
        "for i in range(NUM_SAMPLES):\n",
        "    image_filename = os.path.join(images_dir, f'image_{i:03d}.npy')\n",
        "    mask_filename = os.path.join(masks_dir, f'mask_{i:03d}.png')\n",
        "\n",
        "    if i == 0: # For the first sample, use the actual Sentinel-2 GeoTIFFs\n",
        "        print(f\"Processing actual Sentinel-2 GeoTIFFs for sample {i:03d}...\")\n",
        "        stacked_bands = []\n",
        "        for band_key in ['B02', 'B03', 'B04', 'B08']:\n",
        "            band_path = sentinel_band_paths[band_key]\n",
        "            with rasterio.open(band_path) as src:\n",
        "                band_data = src.read(1, out_shape=(1, IMG_HEIGHT, IMG_WIDTH), resampling=rasterio.enums.Resampling.nearest)\n",
        "                stacked_bands.append(band_data)\n",
        "        dummy_image_array = np.stack(stacked_bands, axis=-1)\n",
        "        np.save(image_filename, dummy_image_array)\n",
        "\n",
        "        # Derive mask from actual Sentinel-2 bands using NDVI\n",
        "        b04 = dummy_image_array[:, :, 2].astype(np.float32) # Red band (index 2)\n",
        "        b08 = dummy_image_array[:, :, 3].astype(np.float32) # NIR band (index 3)\n",
        "        numerator = b08 - b04\n",
        "        denominator = b08 + b04\n",
        "        ndvi = np.where(denominator == 0, 0, numerator / denominator)\n",
        "        dummy_mask_array = (ndvi > 0.4).astype(np.uint8) # Threshold for healthy vs. stressed\n",
        "        dummy_mask = Image.fromarray(dummy_mask_array, mode='L')\n",
        "        dummy_mask.save(mask_filename)\n",
        "        print(f\"Successfully processed actual Sentinel-2 image and derived mask for sample {i:03d}.\")\n",
        "    else: # For subsequent samples, generate dummy data as before\n",
        "        # Generate dummy 4-channel, 16-bit image array (simulating Sentinel-2 data)\n",
        "        dummy_image_array = np.random.randint(0, 10001, (IMG_HEIGHT, IMG_WIDTH, 4), dtype=np.uint16)\n",
        "        np.save(image_filename, dummy_image_array)\n",
        "\n",
        "        # Derive mask from NDVI values of dummy data\n",
        "        b04 = dummy_image_array[:, :, 2].astype(np.float32) # Red band (index 2)\n",
        "        b08 = dummy_image_array[:, :, 3].astype(np.float32) # NIR band (index 3)\n",
        "        numerator = b08 - b04\n",
        "        denominator = b08 + b04\n",
        "        ndvi = np.where(denominator == 0, 0, numerator / denominator)\n",
        "        dummy_mask_array = (ndvi > 0.4).astype(np.uint8)\n",
        "        dummy_mask = Image.fromarray(dummy_mask_array, mode='L')\n",
        "        dummy_mask.save(mask_filename)\n",
        "\n",
        "print(f\"Successfully generated {NUM_SAMPLES} image (.npy) and NDVI-derived mask (.png) pairs in '{BASE_DIR}'.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf7eae83"
      },
      "source": [
        "### 2. Create TensorFlow Training and Validation Datasets\n",
        "\n",
        "This step defines a `load_image_mask` function to read the `.npy` image files and `.png` mask files, apply Sentinel-2 specific normalization (dividing by 10000.0 for images), and then creates `tf.data.Dataset` objects. The datasets are split into training and validation sets, shuffled, batched, and prefetched for optimized performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1dc4aa6"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np # numpy is needed to load .npy files\n",
        "\n",
        "# Define image dimensions and batch size (already defined, but including for completeness)\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "BATCH_SIZE = 32\n",
        "BASE_DIR = 'unet_dataset'\n",
        "\n",
        "# Define paths for images and masks\n",
        "images_dir = os.path.join(BASE_DIR, 'images')\n",
        "masks_dir = os.path.join(BASE_DIR, 'masks')\n",
        "\n",
        "# Function to load and preprocess image and mask, updated for .npy images\n",
        "def load_image_mask(image_path, mask_path):\n",
        "    # Load image (now .npy files)\n",
        "    image = tf.py_function(lambda x: np.load(x.numpy()), [image_path], tf.uint16)\n",
        "    image.set_shape([IMG_HEIGHT, IMG_WIDTH, 4]) # Ensure shape is defined\n",
        "    image = tf.cast(image, tf.float32) / 10000.0 # Rescale to [0, 1] for Sentinel-2 data\n",
        "\n",
        "    # Load mask (still .png files)\n",
        "    mask = tf.io.read_file(mask_path)\n",
        "    mask = tf.image.decode_png(mask, channels=1)\n",
        "    mask = tf.image.resize(mask, [IMG_HEIGHT, IMG_WIDTH], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    mask = tf.cast(mask, tf.uint8) # Masks typically kept as int/uint for class labels\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "# Get lists of image and mask paths (updated to look for .npy files for images)\n",
        "image_paths = sorted([os.path.join(images_dir, fname) for fname in os.listdir(images_dir) if fname.endswith('.npy')])\n",
        "mask_paths = sorted([os.path.join(masks_dir, fname) for fname in os.listdir(masks_dir) if fname.endswith('.png')])\n",
        "\n",
        "# Create a TensorFlow Dataset from all samples\n",
        "full_dataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))\n",
        "full_dataset = full_dataset.map(load_image_mask, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Determine dataset size for splitting\n",
        "DATASET_SIZE = len(image_paths)\n",
        "TRAIN_SIZE = int(0.8 * DATASET_SIZE)\n",
        "VAL_SIZE = DATASET_SIZE - TRAIN_SIZE\n",
        "\n",
        "# Split the dataset into training and validation sets before batching\n",
        "train_dataset_raw = full_dataset.take(TRAIN_SIZE)\n",
        "val_dataset_raw = full_dataset.skip(TRAIN_SIZE)\n",
        "\n",
        "# Apply shuffle, batch, and prefetch to training dataset\n",
        "train_dataset = train_dataset_raw.shuffle(buffer_size=TRAIN_SIZE if TRAIN_SIZE > 0 else 1).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# Apply batch and prefetch to validation dataset (shuffling not strictly necessary for validation)\n",
        "val_dataset = val_dataset_raw.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "print(f\"Successfully re-created training and validation datasets for Sentinel-like data. Number of training batches: {tf.data.experimental.cardinality(train_dataset).numpy()}, Number of validation batches: {tf.data.experimental.cardinality(val_dataset).numpy()}.\")\n",
        "\n",
        "# Optional: inspect a batch\n",
        "for image_batch, mask_batch in train_dataset.take(1):\n",
        "    print(f\"First training image batch shape: {image_batch.shape}\")\n",
        "    print(f\"First training mask batch shape: {mask_batch.shape}\")\n",
        "    break # Only inspect one batch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc49a29e"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The data pipeline was successfully modified to accommodate Sentinel-2 multispectral data. This involved generating dummy 4-channel, 16-bit `.npy` image files, updating the `load_image_mask` function to read these files and normalize them by dividing by 10000.0, and then re-creating the training and validation datasets using this updated logic. The successful re-creation was confirmed by verifying the shapes and batch counts of the resulting datasets.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   Ten dummy 4-channel, 16-bit NumPy arrays, each with dimensions `(128, 128, 4)` and containing `uint16` values ranging from 0 to 10000, were generated as `.npy` files to simulate Sentinel-2 multispectral data.\n",
        "*   The `load_image_mask` function was updated to read these `.npy` image files, specifically using `tf.py_function` with `np.load`, set the image shape to `[128, 128, 4]`, and normalize the image data by dividing by 10000.0.\n",
        "*   The `train_dataset` and `val_dataset` were successfully re-created, with the `train_dataset` containing 1 batch and the `val_dataset` also containing 1 batch, given a batch size of 32 and 10 total samples (8 for training, 2 for validation).\n",
        "*   Verification of a training batch confirmed the correct shapes: image batches were `(8, 128, 128, 4)` and mask batches were `(8, 128, 128, 1)`.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The updated data pipeline is now ready for training a model with Sentinel-2-like multispectral data, ensuring proper loading, preprocessing, and normalization.\n",
        "*   The next step would be to integrate this `train_dataset` and `val_dataset` into a model training loop to verify the full end-to-end functionality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e55151f"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the `tf.data.AUTOTUNE` value for `buffer_size` in the `shuffle()` method can result in a non-positive value or an 'UNKNOWN_CARDINALITY' error for small datasets. I will set `BUFFER_SIZE` to a concrete positive integer equal to `NUM_SAMPLES` to ensure proper shuffling and resolve the `InvalidArgumentError`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3978b4fc"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define image dimensions and batch size (already defined, but including for completeness)\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "BATCH_SIZE = 32\n",
        "BASE_DIR = 'unet_dataset'\n",
        "\n",
        "# Define paths for images and masks\n",
        "images_dir = os.path.join(BASE_DIR, 'images')\n",
        "masks_dir = os.path.join(BASE_DIR, 'masks')\n",
        "\n",
        "# Function to load and preprocess image and mask\n",
        "def load_image_mask(image_path, mask_path):\n",
        "    # Load image\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])\n",
        "    image = tf.cast(image, tf.float32) / 255.0 # Rescale to [0, 1]\n",
        "\n",
        "    # Load mask\n",
        "    mask = tf.io.read_file(mask_path)\n",
        "    # Use decode_png for masks as they were saved as PNG\n",
        "    mask = tf.image.decode_png(mask, channels=1)\n",
        "    mask = tf.image.resize(mask, [IMG_HEIGHT, IMG_WIDTH], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    mask = tf.cast(mask, tf.uint8) # Masks typically kept as int/uint for class labels\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "# Get lists of image and mask paths\n",
        "image_paths = sorted([os.path.join(images_dir, fname) for fname in os.listdir(images_dir) if fname.endswith('.jpg')])\n",
        "mask_paths = sorted([os.path.join(masks_dir, fname) for fname in os.listdir(masks_dir) if fname.endswith('.png')])\n",
        "\n",
        "# Create a TensorFlow Dataset from all samples\n",
        "full_dataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))\n",
        "full_dataset = full_dataset.map(load_image_mask, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Determine dataset size for splitting\n",
        "DATASET_SIZE = len(image_paths)\n",
        "TRAIN_SIZE = int(0.8 * DATASET_SIZE)\n",
        "VAL_SIZE = DATASET_SIZE - TRAIN_SIZE\n",
        "\n",
        "# Split the dataset into training and validation sets before batching\n",
        "train_dataset_raw = full_dataset.take(TRAIN_SIZE)\n",
        "val_dataset_raw = full_dataset.skip(TRAIN_SIZE)\n",
        "\n",
        "# Apply shuffle, batch, and prefetch to training dataset\n",
        "train_dataset = train_dataset_raw.shuffle(buffer_size=TRAIN_SIZE if TRAIN_SIZE > 0 else 1).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# Apply batch and prefetch to validation dataset (shuffling not strictly necessary for validation)\n",
        "val_dataset = val_dataset_raw.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "print(f\"Successfully created training and validation datasets. Number of training batches: {tf.data.experimental.cardinality(train_dataset).numpy()}, Number of validation batches: {tf.data.experimental.cardinality(val_dataset).numpy()}.\")\n",
        "\n",
        "# Optional: inspect a batch\n",
        "# for image_batch, mask_batch in train_dataset.take(1):\n",
        "#     print(f\"Image batch shape: {image_batch.shape}\")\n",
        "#     print(f\"Mask batch shape: {mask_batch.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac9af81a"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the completion of the U-Net segmentation setup, including the creation of dummy image-mask pairs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95e00cc0"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Dummy Data Generation**: Successfully created 10 dummy JPEG images and 10 corresponding PNG mask files within the `unet_dataset/images` and `unet_dataset/masks` directories, respectively.\n",
        "*   **TensorFlow Dataset Creation Error**: An initial attempt to create the TensorFlow `train_dataset` and `val_dataset` failed with an `InvalidArgumentError` related to `buffer_size` in the `dataset.shuffle()` method. This error occurred because `tf.data.AUTOTUNE` likely resulted in an invalid buffer size for the small dataset.\n",
        "*   **Corrected Dataset Creation**: The `buffer_size` for the `shuffle()` operation was explicitly set to `TRAIN_SIZE`, resolving the error. Subsequently, the data pipeline successfully loaded, preprocessed, shuffled, batched, and prefetched the data, resulting in training and validation datasets with 1 batch each.\n",
        "*   **U-Net Setup Completion**: The overall U-Net segmentation setup, including dummy data generation and the creation of a TensorFlow data pipeline, was successfully completed.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   When working with small datasets, explicitly define `buffer_size` for `tf.data.Dataset.shuffle()` rather than relying solely on `tf.data.AUTOTUNE` to avoid potential `InvalidArgumentError`.\n",
        "*   Proceed with defining and training the U-Net model using the established `train_dataset` and `val_dataset`.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yMPg5jW2MPPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3943f6e"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the dummy Sentinel-2-like .npy image files and .png mask files have been generated, the next step is to update the `load_image_mask` function to correctly read these .npy files, normalize the pixel values by 10000.0 (as specified for Sentinel-2 data), and then re-create the `train_dataset` and `val_dataset` using this modified function and the newly generated file paths.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96637e0d"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np # numpy is needed to load .npy files\n",
        "\n",
        "# Define image dimensions and batch size (already defined, but including for completeness)\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "BATCH_SIZE = 32\n",
        "BASE_DIR = 'unet_dataset'\n",
        "\n",
        "# Define paths for images and masks\n",
        "images_dir = os.path.join(BASE_DIR, 'images')\n",
        "masks_dir = os.path.join(BASE_DIR, 'masks')\n",
        "\n",
        "# Function to load and preprocess image and mask, updated for .npy images\n",
        "def load_image_mask(image_path, mask_path):\n",
        "    # Load image (now .npy files)\n",
        "    image = tf.py_function(lambda x: np.load(x.numpy()), [image_path], tf.uint16)\n",
        "    image.set_shape([IMG_HEIGHT, IMG_WIDTH, 4]) # Ensure shape is defined\n",
        "    image = tf.cast(image, tf.float32) / 10000.0 # Rescale to [0, 1] for Sentinel-2 data\n",
        "\n",
        "    # Load mask (still .png files)\n",
        "    mask = tf.io.read_file(mask_path)\n",
        "    mask = tf.image.decode_png(mask, channels=1)\n",
        "    mask = tf.image.resize(mask, [IMG_HEIGHT, IMG_WIDTH], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    mask = tf.cast(mask, tf.uint8) # Masks typically kept as int/uint for class labels\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "# Get lists of image and mask paths (updated to look for .npy files for images)\n",
        "image_paths = sorted([os.path.join(images_dir, fname) for fname in os.listdir(images_dir) if fname.endswith('.npy')])\n",
        "mask_paths = sorted([os.path.join(masks_dir, fname) for fname in os.listdir(masks_dir) if fname.endswith('.png')])\n",
        "\n",
        "# Create a TensorFlow Dataset from all samples\n",
        "full_dataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))\n",
        "full_dataset = full_dataset.map(load_image_mask, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Determine dataset size for splitting\n",
        "DATASET_SIZE = len(image_paths)\n",
        "TRAIN_SIZE = int(0.8 * DATASET_SIZE)\n",
        "VAL_SIZE = DATASET_SIZE - TRAIN_SIZE\n",
        "\n",
        "# Split the dataset into training and validation sets before batching\n",
        "train_dataset_raw = full_dataset.take(TRAIN_SIZE)\n",
        "val_dataset_raw = full_dataset.skip(TRAIN_SIZE)\n",
        "\n",
        "# Apply shuffle, batch, and prefetch to training dataset\n",
        "train_dataset = train_dataset_raw.shuffle(buffer_size=TRAIN_SIZE if TRAIN_SIZE > 0 else 1).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# Apply batch and prefetch to validation dataset (shuffling not strictly necessary for validation)\n",
        "val_dataset = val_dataset_raw.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "print(f\"Successfully re-created training and validation datasets for Sentinel-like data. Number of training batches: {tf.data.experimental.cardinality(train_dataset).numpy()}, Number of validation batches: {tf.data.experimental.cardinality(val_dataset).numpy()}.\")\n",
        "\n",
        "# Optional: inspect a batch\n",
        "for image_batch, mask_batch in train_dataset.take(1):\n",
        "    print(f\"First training image batch shape: {image_batch.shape}\")\n",
        "    print(f\"First training mask batch shape: {mask_batch.shape}\")\n",
        "    break # Only inspect one batch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9126d95"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the changes made to the data pipeline to accommodate Sentinel-2 multispectral data and confirm the successful re-creation of the training and validation datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8b395b2"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The data pipeline was successfully modified to accommodate Sentinel-2 multispectral data. This involved generating dummy 4-channel, 16-bit `.npy` image files, updating the `load_image_mask` function to read these files and normalize them by dividing by 10000.0, and then re-creating the training and validation datasets using this updated logic. The successful re-creation was confirmed by verifying the shapes and batch counts of the resulting datasets.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   Ten dummy 4-channel, 16-bit NumPy arrays, each with dimensions `(128, 128, 4)` and containing `uint16` values ranging from 0 to 10000, were generated as `.npy` files to simulate Sentinel-2 multispectral data.\n",
        "*   The `load_image_mask` function was updated to read these `.npy` image files, specifically using `tf.py_function` with `np.load`, set the image shape to `[128, 128, 4]`, and normalize the image data by dividing by 10000.0.\n",
        "*   The `train_dataset` and `val_dataset` were successfully re-created, with the `train_dataset` containing 1 batch and the `val_dataset` also containing 1 batch, given a batch size of 32 and 10 total samples (8 for training, 2 for validation).\n",
        "*   Verification of a training batch confirmed the correct shapes: image batches were `(8, 128, 128, 4)` and mask batches were `(8, 128, 128, 1)`.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The updated data pipeline is now ready for training a model with Sentinel-2-like multispectral data, ensuring proper loading, preprocessing, and normalization.\n",
        "*   The next step would be to integrate this `train_dataset` and `val_dataset` into a model training loop to verify the full end-to-end functionality.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6d64e49"
      },
      "source": [
        "pip install rasterio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4f108f2"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that `rasterio` is installed, I will modify the previous code to load the provided Sentinel-2 GeoTIFF files, stack the B02, B03, B04, and B08 bands to create a 4-channel multispectral image, and save it as a `.npy` file. I will also keep the dummy mask generation for now, as no specific masks for these GeoTIFFs were provided. This ensures the data pipeline uses actual (though placeholder-masked) Sentinel-2-like data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8571872"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import shutil\n",
        "import rasterio # Re-import rasterio\n",
        "\n",
        "# Define constants\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "NUM_SAMPLES = 50 # Keep the increased number of samples for adequate dataset size\n",
        "BASE_DIR = 'unet_dataset'\n",
        "\n",
        "# Define paths for images and masks\n",
        "images_dir = os.path.join(BASE_DIR, 'images')\n",
        "masks_dir = os.path.join(BASE_DIR, 'masks')\n",
        "\n",
        "# Ensure a clean slate: remove existing directories if they exist\n",
        "if os.path.exists(BASE_DIR):\n",
        "    shutil.rmtree(BASE_DIR)\n",
        "    print(f\"Removed existing directory: {BASE_DIR}\")\n",
        "\n",
        "# Create base directory and subdirectories for images and masks\n",
        "os.makedirs(images_dir, exist_ok=True)\n",
        "os.makedirs(masks_dir, exist_ok=True)\n",
        "print(f\"Created directories: {images_dir} and {masks_dir}\")\n",
        "\n",
        "# Paths to the provided Sentinel-2 GeoTIFF files\n",
        "sentinel_band_paths = {\n",
        "    'B02': '/content/2023-02-04-00_00_2023-05-25-23_59_Sentinel-2_L1C_B02_(Raw).tiff',\n",
        "    'B03': '/content/2023-02-04-00_00_2023-05-25-23_59_Sentinel-2_L1C_B03_(Raw).tiff',\n",
        "    'B04': '/content/2023-02-04-00_00_2023-05-25-23_59_Sentinel-2_L1C_B04_(Raw).tiff',\n",
        "    'B08': '/content/2023-02-04-00_00_2023-05-25-23_59_Sentinel-2_L1C_B08_(Raw).tiff'\n",
        "}\n",
        "\n",
        "# Generate dummy image and NDVI-derived mask files\n",
        "for i in range(NUM_SAMPLES):\n",
        "    image_filename = os.path.join(images_dir, f'image_{i:03d}.npy')\n",
        "    mask_filename = os.path.join(masks_dir, f'mask_{i:03d}.png')\n",
        "\n",
        "    if i == 0: # For the first sample, use the actual Sentinel-2 GeoTIFFs\n",
        "        print(f\"Processing actual Sentinel-2 GeoTIFFs for sample {i:03d}...\")\n",
        "        stacked_bands = []\n",
        "        for band_key in ['B02', 'B03', 'B04', 'B08']:\n",
        "            band_path = sentinel_band_paths[band_key]\n",
        "            with rasterio.open(band_path) as src:\n",
        "                band_data = src.read(1, out_shape=(1, IMG_HEIGHT, IMG_WIDTH), resampling=rasterio.enums.Resampling.nearest)\n",
        "                stacked_bands.append(band_data)\n",
        "        dummy_image_array = np.stack(stacked_bands, axis=-1)\n",
        "        np.save(image_filename, dummy_image_array)\n",
        "\n",
        "        # Derive mask from actual Sentinel-2 bands\n",
        "        b04 = dummy_image_array[:, :, 2].astype(np.float32) # Red band (index 2)\n",
        "        b08 = dummy_image_array[:, :, 3].astype(np.float32) # NIR band (index 3)\n",
        "        numerator = b08 - b04\n",
        "        denominator = b08 + b04\n",
        "        ndvi = np.where(denominator == 0, 0, numerator / denominator)\n",
        "        dummy_mask_array = (ndvi > 0.4).astype(np.uint8) # Threshold for healthy vs. stressed\n",
        "        dummy_mask = Image.fromarray(dummy_mask_array, mode='L')\n",
        "        dummy_mask.save(mask_filename)\n",
        "        print(f\"Successfully processed actual Sentinel-2 image and derived mask for sample {i:03d}.\")\n",
        "    else: # For subsequent samples, generate dummy data as before\n",
        "        # Generate dummy 4-channel, 16-bit image array (simulating Sentinel-2 data)\n",
        "        dummy_image_array = np.random.randint(0, 10001, (IMG_HEIGHT, IMG_WIDTH, 4), dtype=np.uint16)\n",
        "        np.save(image_filename, dummy_image_array)\n",
        "\n",
        "        # Derive mask from NDVI values of dummy data\n",
        "        b04 = dummy_image_array[:, :, 2].astype(np.float32) # Red band (index 2)\n",
        "        b08 = dummy_image_array[:, :, 3].astype(np.float32) # NIR band (index 3)\n",
        "        numerator = b08 - b04\n",
        "        denominator = b08 + b04\n",
        "        ndvi = np.where(denominator == 0, 0, numerator / denominator)\n",
        "        dummy_mask_array = (ndvi > 0.4).astype(np.uint8)\n",
        "        dummy_mask = Image.fromarray(dummy_mask_array, mode='L')\n",
        "        dummy_mask.save(mask_filename)\n",
        "\n",
        "print(f\"Successfully generated {NUM_SAMPLES} image (.npy) and NDVI-derived mask (.png) pairs in '{BASE_DIR}'.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fca7b4b1"
      },
      "source": [
        "# Task\n",
        "Modify the data pipeline to handle Sentinel-2 multispectral data by replacing existing dummy JPG images with 4-channel, 16-bit dummy .npy image files, updating the `load_image_mask` function to read these .npy files and normalize them by 10000.0, and then re-create the `train_dataset` and `val_dataset` to ensure the pipeline is correctly configured for Sentinel-2 data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0941e61a"
      },
      "source": [
        "## Generate Dummy Multispectral Sentinel-like Data\n",
        "\n",
        "### Subtask:\n",
        "Create new dummy image files simulating Sentinel-2 data (e.g., 4 channels, 16-bit data range) and save them as .npy files. Existing dummy PNG masks will remain the same. This step will replace the current JPG dummy image generation to correctly mimic Sentinel-2 data properties.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "175cdc93"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires generating new dummy image files that simulate Sentinel-2 data (4 channels, 16-bit, saved as .npy) while keeping the existing PNG masks. This code block will import necessary libraries, define constants, ensure a clean directory structure, and then loop to create the specified dummy image and mask files according to the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84b96a57"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import shutil\n",
        "\n",
        "# Define constants\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "NUM_SAMPLES = 10\n",
        "BASE_DIR = 'unet_dataset'\n",
        "\n",
        "# Define paths for images and masks\n",
        "images_dir = os.path.join(BASE_DIR, 'images')\n",
        "masks_dir = os.path.join(BASE_DIR, 'masks')\n",
        "\n",
        "# Ensure a clean slate: remove existing directories if they exist\n",
        "if os.path.exists(BASE_DIR):\n",
        "    shutil.rmtree(BASE_DIR)\n",
        "    print(f\"Removed existing directory: {BASE_DIR}\")\n",
        "\n",
        "# Create base directory and subdirectories for images and masks\n",
        "os.makedirs(images_dir, exist_ok=True)\n",
        "os.makedirs(masks_dir, exist_ok=True)\n",
        "print(f\"Created directories: {images_dir} and {masks_dir}\")\n",
        "\n",
        "# Generate dummy image and mask files\n",
        "for i in range(NUM_SAMPLES):\n",
        "    # a. Generate dummy 4-channel, 16-bit image array (simulating Sentinel-2 data)\n",
        "    # Values between 0 and 10000 to mimic common Sentinel-2 scaled reflectance values\n",
        "    dummy_image_array = np.random.randint(0, 10001, (IMG_HEIGHT, IMG_WIDTH, 4), dtype=np.uint16)\n",
        "\n",
        "    # b. Define the image filename within images_dir\n",
        "    image_filename = os.path.join(images_dir, f'image_{i:03d}.npy')\n",
        "\n",
        "    # c. Save the dummy image array as a .npy file\n",
        "    np.save(image_filename, dummy_image_array)\n",
        "\n",
        "    # d. Generate a dummy 1-channel, 8-bit mask array\n",
        "    # Assuming 2 classes (0 or 1)\n",
        "    dummy_mask_array = np.random.randint(0, 2, (IMG_HEIGHT, IMG_WIDTH), dtype=np.uint8)\n",
        "\n",
        "    # e. Convert the mask array to a PIL Image in 'L' mode\n",
        "    dummy_mask = Image.fromarray(dummy_mask_array, mode='L')\n",
        "\n",
        "    # f. Define the mask filename within masks_dir\n",
        "    mask_filename = os.path.join(masks_dir, f'mask_{i:03d}.png')\n",
        "\n",
        "    # g. Save the dummy mask image as a .png file\n",
        "    dummy_mask.save(mask_filename)\n",
        "\n",
        "print(f\"Successfully generated {NUM_SAMPLES} dummy Sentinel-like image (.npy) and mask (.png) pairs in '{BASE_DIR}'.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af94108a"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the dummy Sentinel-2-like .npy image files and .png mask files have been generated, the next step is to update the `load_image_mask` function to correctly read these .npy files, normalize the pixel values by 10000.0 (as specified for Sentinel-2 data), and then re-create the `train_dataset` and `val_dataset` using this modified function and the newly generated file paths.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "551f2a17"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np # numpy is needed to load .npy files\n",
        "\n",
        "# Define image dimensions and batch size (already defined, but including for completeness)\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "BATCH_SIZE = 32\n",
        "BASE_DIR = 'unet_dataset'\n",
        "\n",
        "# Define paths for images and masks\n",
        "images_dir = os.path.join(BASE_DIR, 'images')\n",
        "masks_dir = os.path.join(BASE_DIR, 'masks')\n",
        "\n",
        "# Function to load and preprocess image and mask, updated for .npy images\n",
        "def load_image_mask(image_path, mask_path):\n",
        "    # Load image (now .npy files)\n",
        "    image = tf.py_function(lambda x: np.load(x.numpy()), [image_path], tf.uint16)\n",
        "    image.set_shape([IMG_HEIGHT, IMG_WIDTH, 4]) # Ensure shape is defined\n",
        "    image = tf.cast(image, tf.float32) / 10000.0 # Rescale to [0, 1] for Sentinel-2 data\n",
        "\n",
        "    # Load mask (still .png files)\n",
        "    mask = tf.io.read_file(mask_path)\n",
        "    mask = tf.image.decode_png(mask, channels=1)\n",
        "    mask = tf.image.resize(mask, [IMG_HEIGHT, IMG_WIDTH], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    mask = tf.cast(mask, tf.uint8) # Masks typically kept as int/uint for class labels\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "# Get lists of image and mask paths (updated to look for .npy files for images)\n",
        "image_paths = sorted([os.path.join(images_dir, fname) for fname in os.listdir(images_dir) if fname.endswith('.npy')])\n",
        "mask_paths = sorted([os.path.join(masks_dir, fname) for fname in os.listdir(masks_dir) if fname.endswith('.png')])\n",
        "\n",
        "# Create a TensorFlow Dataset from all samples\n",
        "full_dataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))\n",
        "full_dataset = full_dataset.map(load_image_mask, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Determine dataset size for splitting\n",
        "DATASET_SIZE = len(image_paths)\n",
        "TRAIN_SIZE = int(0.8 * DATASET_SIZE)\n",
        "VAL_SIZE = DATASET_SIZE - TRAIN_SIZE\n",
        "\n",
        "# Split the dataset into training and validation sets before batching\n",
        "train_dataset_raw = full_dataset.take(TRAIN_SIZE)\n",
        "val_dataset_raw = full_dataset.skip(TRAIN_SIZE)\n",
        "\n",
        "# Apply shuffle, batch, and prefetch to training dataset\n",
        "train_dataset = train_dataset_raw.shuffle(buffer_size=TRAIN_SIZE if TRAIN_SIZE > 0 else 1).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# Apply batch and prefetch to validation dataset (shuffling not strictly necessary for validation)\n",
        "val_dataset = val_dataset_raw.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "print(f\"Successfully re-created training and validation datasets for Sentinel-like data. Number of training batches: {tf.data.experimental.cardinality(train_dataset).numpy()}, Number of validation batches: {tf.data.experimental.cardinality(val_dataset).numpy()}.\")\n",
        "\n",
        "# Optional: inspect a batch\n",
        "for image_batch, mask_batch in train_dataset.take(1):\n",
        "    print(f\"First training image batch shape: {image_batch.shape}\")\n",
        "    print(f\"First training mask batch shape: {mask_batch.shape}\")\n",
        "    break # Only inspect one batch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52001b7c"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the changes made to the data pipeline to accommodate Sentinel-2 multispectral data and confirm the successful re-creation of the training and validation datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d62ced3f"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The data pipeline was successfully modified to accommodate Sentinel-2 multispectral data. This involved generating dummy 4-channel, 16-bit `.npy` image files, updating the `load_image_mask` function to read these files and normalize them by dividing by 10000.0, and then re-creating the training and validation datasets using this updated logic. The successful re-creation was confirmed by verifying the shapes and batch counts of the resulting datasets.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   Ten dummy 4-channel, 16-bit NumPy arrays, each with dimensions `(128, 128, 4)` and containing `uint16` values ranging from 0 to 10000, were generated as `.npy` files to simulate Sentinel-2 multispectral data.\n",
        "*   The `load_image_mask` function was updated to read these `.npy` image files, specifically using `tf.py_function` with `np.load`, set the image shape to `[128, 128, 4]`, and normalize the image data by dividing by 10000.0.\n",
        "*   The `train_dataset` and `val_dataset` were successfully re-created, with the `train_dataset` containing 1 batch and the `val_dataset` also containing 1 batch, given a batch size of 32 and 10 total samples (8 for training, 2 for validation).\n",
        "*   Verification of a training batch confirmed the correct shapes: image batches were `(8, 128, 128, 4)` and mask batches were `(8, 128, 128, 1)`.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The updated data pipeline is now ready for training a model with Sentinel-2-like multispectral data, ensuring proper loading, preprocessing, and normalization.\n",
        "*   The next step would be to integrate this `train_dataset` and `val_dataset` into a model training loop to verify the full end-to-end functionality.\n"
      ]
    }
  ]
}